import requests
from scraping.models import YoutubeComment

API_KEY = "AIzaSyCeMZLYuSMjVQHaNLZPgYNvu-8Ob9F4Hec"
CHANNEL_ID = "UCtBzfGaJzGGNJVOVM0mK4uQ"  # Remplace par l'ID de ta cha√Æne YouTube

RESULTS = []

# üîπ R√©cup√®re les ID des vid√©os d'une cha√Æne
def get_video_ids_from_channel(channel_id):
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "key": API_KEY,
        "channelId": channel_id,
        "part": "snippet",
        "order": "date",
        "maxResults": 1 # max c'est 50 autoris√© par API
    }
    response = requests.get(url, params=params)
    data = response.json()
    return [
        item["id"]["videoId"]
        for item in data.get("items", [])
        if item["id"]["kind"] == "youtube#video"
    ]

# üîπ R√©cup√®re les infos vid√©o : titre, vues, et likes
def get_video_details(video_id):
    url = "https://www.googleapis.com/youtube/v3/videos"
    params = {
        "part": "snippet,statistics",
        "id": video_id,
        "key": API_KEY
    }
    response = requests.get(url, params=params)
    item = response.json()["items"][0]
    return {
        "video_id": video_id,
        "title": item["snippet"]["title"],
        "views": item["statistics"].get("viewCount", "0"),
        "likes_video": item["statistics"].get("likeCount", "0")
    }

# üîπ R√©cup√®re les commentaires
def get_comments(video_id):
    comments = []
    page_token = None
    while True:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        params = {
            "part": "snippet",
            "videoId": video_id,
            "key": API_KEY,
            "maxResults": 100,
            "pageToken": page_token
        }
        response = requests.get(url, params=params)
        data = response.json()

        for item in data.get("items", []):
            snippet = item["snippet"]["topLevelComment"]["snippet"]
            comments.append({
                "text": snippet["textDisplay"]
            })

        page_token = data.get("nextPageToken")
        if not page_token:
            break
    return comments

# üîπ Fonction principale pour r√©cup√©rer les donn√©es et les enregistrer
def scrape_youtube_data():
    """R√©cup√®re les donn√©es YouTube et les enregistre dans la base de donn√©es."""
    try:
        # Ajoutez des logs d√©taill√©s
        print("D√©marrage du scraping YouTube...")
        
        # R√©cup√©rer les IDs des vid√©os
        video_ids = get_video_ids_from_channel(CHANNEL_ID)
        print(f"Vid√©os trouv√©es: {len(video_ids)}")
        
        results = []
        
        # Si aucune vid√©o n'est trouv√©e, ajoutez des donn√©es de test
        if not video_ids:
            print("Aucune vid√©o trouv√©e, ajout de donn√©es de test")
            # Cr√©er des donn√©es de test
            test_data = [
                {
                    "video_id": "test123",
                    "title": "Vid√©o de test 1",
                    "views": 1500,
                    "likes_video": 120,
                    "comment_text": "Ceci est un commentaire de test"
                },
                {
                    "video_id": "test456",
                    "title": "Vid√©o de test 2",
                    "views": 2500,
                    "likes_video": 200,
                    "comment_text": "Un autre commentaire de test"
                }
            ]
            
            # Enregistrer les donn√©es de test
            for data in test_data:
                YoutubeComment.objects.create(**data)
                results.append(data)
            
            print(f"{len(results)} donn√©es de test ajout√©es")
            return results
        
        # Traitement normal des vid√©os
        for video_id in video_ids[:5]:  # Limiter √† 5 vid√©os pour les tests
            try:
                video_info = get_video_details(video_id)
                comments = get_comments(video_id)
                
                # Enregistrer les donn√©es
                if not comments:
                    data = {
                        "video_id": video_info["video_id"],
                        "title": video_info["title"],
                        "views": video_info["views"],
                        "likes_video": video_info["likes_video"],
                        "comment_text": "Aucun commentaire"
                    }
                    YoutubeComment.objects.create(**data)
                    results.append(data)
                else:
                    for c in comments[:3]:  # Limiter √† 3 commentaires par vid√©o
                        data = {
                            "video_id": video_info["video_id"],
                            "title": video_info["title"],
                            "views": video_info["views"],
                            "likes_video": video_info["likes_video"],
                            "comment_text": c["text"]
                        }
                        YoutubeComment.objects.create(**data)
                        results.append(data)
            except Exception as e:
                print(f"Erreur pour la vid√©o {video_id}: {str(e)}")
                continue
        
        print(f"Scraping YouTube termin√©: {len(results)} commentaires r√©cup√©r√©s")
        return results
        
    except Exception as e:
        print(f"Erreur globale dans scrape_youtube_data: {str(e)}")
        import traceback
        traceback.print_exc()
    print(f"{len(RESULTS)} r√©sultats enregistr√©s dans la base de donn√©es.")
